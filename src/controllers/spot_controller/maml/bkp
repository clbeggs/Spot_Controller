# import asyncio
import copy

import numpy as np
import torch


class Buffer():
    """ """
    def __init__(self, K):
        #self.logp = np.zeros((num_epochs, K, run_length))
        #self.rew = np.zeros((num_epochs, K, run_length))
        self.logp = []
        self.rew = np.zeros((K))
        self.ptr = 0

    def _get_moving_avg(self):
        """Numpy moving average
        Reference: https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-numpy-scipy
        """
        return np.convolve(self.rew, np.ones(self.len), 'valid') / self.rew

    def store(self, reward, logp):
        """Store reward, return moving avg """
        self.rew[self.ptr](reward)
        self.logp.append(reward)
        self.ptr += 1

    def run_finished(self):
        """
            Ref:
            https://towardsdatascience.com/policy-gradient-methods-104c783251e0
        """

        cumulative_rew = self.rew.cumsum()
        for i, prob in enumerate(self.logp):
            prob *= cumulative_rew[i]
            prob.backward()
            # TODO: Sum gradients?

        return self.logp



    def exp_reward(self):
        pass

    def batch_prep(self):
        self.epoch += 1


class Solver():
    """ """
    def __init__(self, model, algo, robot):
        self.model_copy = None
        self.model = model
        self.robot = robot
        self.algo = algo
        self.buffer = Buffer()
        self.gamma = 0.9

    def get_update(self, grads):
        pass

    def task_rollout(self, task):
        """Given task, try and get to goal point"""
        cumuluative_reward = 0

        for i in range(run_length):
            # Get input to model
            obs = self.robot.get_obs()

            # Get action and log prob of action
            action, logp = self.model.step(obs)

            step_result, terminal = self.robot.action_rollout(obs)

            reward = self.robot.get_reward(obs, terminal)

            self.buffer.store(reward * self.gamma**i, logp)

            if (terminal is True) or (step_result == -1):
                # MAKE IT CUMULATIVE!!!
                grads = self.buffer.run_finished()
                return grads

    def train_on_task(self, K, task):
        single_run_update_vals = []
        for _ in range(K):
            grads = self.task_rollout(task)
            update_vals = self.get_update(grads)
            single_run_update_vals.append(update_vals)
        return single_run_update_vals  # TODO: Change this to be in buffer?

    def prep_batch_run(self):
        """Save main model weights"""
        self.model.copy_model()

    def get_batch_of_tasks(self):
        """Return different environments and start/goal points"""
        pass

    def train(self, num_epochs, K):
        for _ in range(num_epochs):
            tasks = self.get_batch_of_tasks()
            self.prep_batch_run()

            for task in tasks:
                self.train_on_task(K, task)


"""Pseudo Code:

tasks: list
step_size_params: tuple



def update_meta(meta-model, list_of_rewards):
    list_of_rewards = [[rew1, rew2, ...], ... ]

    for all models for all task:
        model_sum = sum_for_single_task(model_primes)

        model_prime_cpy = deepcopy(model_prime)
        update_model_prime()



def sample_trajectories(task, K, model);

    model_prime = deepcopy(meta_model)
    for K traj:
        exp_reward = rollout_task(model_prime)

        new_model_prime = update_model_prime(exp_reward)

        exp_reward = rollout_task(new_model_prime)
        rewards.append(exp_reward)
    return rewards


def loss():
    expected_reward: list, final cumuluative reward for each of the K runs for task T_i
    sum = 0
    for each cumuluative_reward of each of K rollouts:
        sum += cumuluative_reward
    return -sum


for num_epochs:
    batch_of_tasks: list = get_batch() # list of length T

    meta_model = deepcopy(model) # NOT TO BE MODIFIED UNTIL update_meta_model()

    list_of_rewards: list = []  # Length T * K
    for task in batch:
        traj = sample_trajectories(task, K)
        rewards_for_task_i: list = eval_trajectories(traj) # list of length K

        list_of_rewards.append(rewards_for_task_i)

        update_model(model)

    update_meta_model(meta_model, list_of_rewards)


"""















